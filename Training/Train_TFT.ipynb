{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price                       Date  Adj Close      Close       High        Low  \\\n",
      "Ticker                                GOOGL      GOOGL      GOOGL      GOOGL   \n",
      "0      2015-01-02 00:00:00+00:00  26.381865  26.477501  26.790001  26.393999   \n",
      "1      2015-01-05 00:00:00+00:00  25.879185  25.973000  26.399500  25.887501   \n",
      "2      2015-01-06 00:00:00+00:00  25.240501  25.332001  26.060499  25.277500   \n",
      "3      2015-01-07 00:00:00+00:00  25.166271  25.257500  25.574499  25.182501   \n",
      "4      2015-01-08 00:00:00+00:00  25.253952  25.345501  25.375000  24.750999   \n",
      "\n",
      "Price        Open    Volume  \n",
      "Ticker      GOOGL     GOOGL  \n",
      "0       26.629999  26480000  \n",
      "1       26.357500  41182000  \n",
      "2       26.025000  54456000  \n",
      "3       25.547501  46918000  \n",
      "4       25.075500  73054000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiIndex([(     'Date',      ''),\n",
       "            ('Adj Close', 'GOOGL'),\n",
       "            (    'Close', 'GOOGL'),\n",
       "            (     'High', 'GOOGL'),\n",
       "            (      'Low', 'GOOGL'),\n",
       "            (     'Open', 'GOOGL'),\n",
       "            (   'Volume', 'GOOGL')],\n",
       "           names=['Price', 'Ticker'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "# Fetch historical data for Google (GOOGL)\n",
    "data_google = yf.download(\"GOOGL\", start=\"2015-01-01\", end=\"2023-01-01\")\n",
    "data_google.reset_index(inplace=True)\n",
    "print(data_google.head())\n",
    "type(data_google)\n",
    "data_google.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2015-01-02 00:00:00+00:00\n",
      "1   2015-01-05 00:00:00+00:00\n",
      "2   2015-01-06 00:00:00+00:00\n",
      "3   2015-01-07 00:00:00+00:00\n",
      "4   2015-01-08 00:00:00+00:00\n",
      "Name: Date, dtype: datetime64[ns, UTC]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiIndex([(    'index',      ''),\n",
       "            (     'Date',      ''),\n",
       "            ('Adj Close', 'GOOGL'),\n",
       "            (    'Close', 'GOOGL'),\n",
       "            (     'High', 'GOOGL'),\n",
       "            (      'Low', 'GOOGL'),\n",
       "            (     'Open', 'GOOGL'),\n",
       "            (   'Volume', 'GOOGL')],\n",
       "           names=['Price', 'Ticker'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_google = data_google.reset_index()\n",
    "print(data_google['Date'].head())\n",
    "data_google.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Date  time  group  Adj Close      Close       High  \\\n",
      "0 2015-01-02 00:00:00+00:00     0  GOOGL  26.381865  26.477501  26.790001   \n",
      "1 2015-01-05 00:00:00+00:00     1  GOOGL  25.879185  25.973000  26.399500   \n",
      "2 2015-01-06 00:00:00+00:00     2  GOOGL  25.240501  25.332001  26.060499   \n",
      "3 2015-01-07 00:00:00+00:00     3  GOOGL  25.166271  25.257500  25.574499   \n",
      "4 2015-01-08 00:00:00+00:00     4  GOOGL  25.253952  25.345501  25.375000   \n",
      "\n",
      "         Low       Open    Volume  \n",
      "0  26.393999  26.629999  26480000  \n",
      "1  25.887501  26.357500  41182000  \n",
      "2  25.277500  26.025000  54456000  \n",
      "3  25.182501  25.547501  46918000  \n",
      "4  24.750999  25.075500  73054000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reset index and flatten the MultiIndex columns\n",
    "data_google = data_google.reset_index()\n",
    "data_google.columns = [col[0] if col[1] == '' else f\"{col[0]}_{col[1]}\" for col in data_google.columns]\n",
    "\n",
    "# Drop the 'index' column if it exists\n",
    "if 'index' in data_google.columns:\n",
    "    data_google = data_google.drop('index', axis=1)\n",
    "\n",
    "# Create time index\n",
    "data_google['time'] = range(len(data_google))\n",
    "\n",
    "# Add group_id and convert to string type\n",
    "data_google['group'] = 'GOOGL'  # Using stock ticker as group identifier\n",
    "\n",
    "# Ensure all columns are properly named\n",
    "columns_to_keep = ['Date', 'time', 'group', 'Adj Close_GOOGL', 'Close_GOOGL', \n",
    "                  'High_GOOGL', 'Low_GOOGL', 'Open_GOOGL', 'Volume_GOOGL']\n",
    "data_google = data_google[columns_to_keep]\n",
    "\n",
    "# Rename columns to remove the ticker suffix\n",
    "data_google.columns = [col.split('_')[0] if '_' in col else col for col in data_google.columns]\n",
    "\n",
    "print(data_google.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting import TimeSeriesDataSet, GroupNormalizer\n",
    "\n",
    "# Define parameters\n",
    "max_prediction_length = 30  # Predict next 30 days\n",
    "max_encoder_length = 120  # Use past 120 days as input\n",
    "training_cutoff = data_google[\"time\"].max() - max_prediction_length\n",
    "\n",
    "# Create the dataset\n",
    "training = TimeSeriesDataSet(\n",
    "    data_google[lambda x: x.time <= training_cutoff],\n",
    "    time_idx=\"time\",\n",
    "    target=\"Close\",\n",
    "    group_ids=[\"group\"],\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"group\"],\n",
    "    static_reals=[],\n",
    "    time_varying_known_categoricals=[],\n",
    "    time_varying_known_reals=[\"time\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"],\n",
    "    target_normalizer=GroupNormalizer(groups=[\"group\"], transformation=\"softplus\"),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "# Create validation dataset\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data_google, min_prediction_idx=training_cutoff + 1)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 64\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deepa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "c:\\Users\\deepa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "c:\\Users\\deepa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_forecasting\\models\\temporal_fusion_transformer\\__init__.py:171: UserWarning: In pytorch-forecasting models, on versions 1.1.X, the default optimizer defaults to 'adam', if pytorch_optimizer is not installed, otherwise it defaults to 'ranger' from pytorch_optimizer. From version 1.2.0, the default optimizer will be 'adam' regardless of whether pytorch_optimizer is installed, in order to minimize the number of dependencies in default parameter settings. Users who wish to ensure their code continues using 'ranger' as optimizer should ensure that pytorch_optimizer is installed, and set the optimizer parameter explicitly to 'ranger'.\n",
      "  super().__init__(loss=loss, logging_metrics=logging_metrics, **kwargs)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\deepa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type                      | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | model | TemporalFusionTransformer | 22.0 K | train\n",
      "------------------------------------------------------------\n",
      "22.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "22.0 K    Total params\n",
      "0.088     Total estimated model params size (MB)\n",
      "356       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deepa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\deepa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 28/28 [00:06<00:00,  4.56it/s, v_num=23, train_loss=2.780, val_loss=17.90]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_forecasting import TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "import torch\n",
    "\n",
    "class StockTFT(LightningModule):\n",
    "    def __init__(self, tft_model):\n",
    "        super().__init__()\n",
    "        self.model = tft_model\n",
    "        # Enable automatic optimization\n",
    "        self.automatic_optimization = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        raw_predictions = self.model(x)\n",
    "        loss = self.model.loss(raw_predictions.prediction, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        raw_predictions = self.model(x)\n",
    "        loss = self.model.loss(raw_predictions.prediction, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=\"min\",\n",
    "            patience=5,\n",
    "            factor=0.5,\n",
    "            verbose=True\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": scheduler,\n",
    "            \"monitor\": \"val_loss\"\n",
    "        }\n",
    "\n",
    "# Modified model configuration\n",
    "tft_params = dict(\n",
    "    hidden_size=16,\n",
    "    attention_head_size=2,\n",
    "    dropout=0.2,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=7,\n",
    "    loss=QuantileLoss(),\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "# Create base model\n",
    "base_tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    **tft_params\n",
    ")\n",
    "\n",
    "# Wrap model\n",
    "tft = StockTFT(base_tft)\n",
    "\n",
    "# Define trainer with modified settings\n",
    "trainer = Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator='cpu',  # Force CPU for stability\n",
    "    devices=1,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\n",
    "    ],\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=1\n",
    ")\n",
    "\n",
    "trainer.fit(tft, train_dataloader, val_dataloader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
