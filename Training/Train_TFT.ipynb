{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price                      Adj Close      Close       High        Low  \\\n",
      "Ticker                         GOOGL      GOOGL      GOOGL      GOOGL   \n",
      "Date                                                                    \n",
      "2015-01-02 00:00:00+00:00  26.381865  26.477501  26.790001  26.393999   \n",
      "2015-01-05 00:00:00+00:00  25.879185  25.973000  26.399500  25.887501   \n",
      "2015-01-06 00:00:00+00:00  25.240501  25.332001  26.060499  25.277500   \n",
      "2015-01-07 00:00:00+00:00  25.166271  25.257500  25.574499  25.182501   \n",
      "2015-01-08 00:00:00+00:00  25.253954  25.345501  25.375000  24.750999   \n",
      "\n",
      "Price                           Open    Volume  \n",
      "Ticker                         GOOGL     GOOGL  \n",
      "Date                                            \n",
      "2015-01-02 00:00:00+00:00  26.629999  26480000  \n",
      "2015-01-05 00:00:00+00:00  26.357500  41182000  \n",
      "2015-01-06 00:00:00+00:00  26.025000  54456000  \n",
      "2015-01-07 00:00:00+00:00  25.547501  46918000  \n",
      "2015-01-08 00:00:00+00:00  25.075500  73054000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiIndex([('Adj Close', 'GOOGL'),\n",
       "            (    'Close', 'GOOGL'),\n",
       "            (     'High', 'GOOGL'),\n",
       "            (      'Low', 'GOOGL'),\n",
       "            (     'Open', 'GOOGL'),\n",
       "            (   'Volume', 'GOOGL')],\n",
       "           names=['Price', 'Ticker'])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "# Fetch historical data for Google (GOOGL)\n",
    "data_google = yf.download(\"GOOGL\", start=\"2015-01-01\", end=\"2023-01-01\")\n",
    "print(data_google.head())\n",
    "type(data_google)\n",
    "data_google.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2015-01-02 00:00:00+00:00\n",
      "1   2015-01-05 00:00:00+00:00\n",
      "2   2015-01-06 00:00:00+00:00\n",
      "3   2015-01-07 00:00:00+00:00\n",
      "4   2015-01-08 00:00:00+00:00\n",
      "Name: Date, dtype: datetime64[ns, UTC]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiIndex([(     'Date',      ''),\n",
       "            ('Adj Close', 'GOOGL'),\n",
       "            (    'Close', 'GOOGL'),\n",
       "            (     'High', 'GOOGL'),\n",
       "            (      'Low', 'GOOGL'),\n",
       "            (     'Open', 'GOOGL'),\n",
       "            (   'Volume', 'GOOGL')],\n",
       "           names=['Price', 'Ticker'])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_google = data_google.reset_index()\n",
    "print(data_google['Date'].head())\n",
    "data_google.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Date  time_idx  group_id  Adj Close      Close  \\\n",
      "0 2015-01-02 00:00:00+00:00         0         0  26.381865        NaN   \n",
      "1 2015-01-02 00:00:00+00:00      2014         0        NaN  26.477501   \n",
      "2 2015-01-02 00:00:00+00:00      4028         0        NaN        NaN   \n",
      "3 2015-01-02 00:00:00+00:00      6042         0        NaN        NaN   \n",
      "4 2015-01-02 00:00:00+00:00      8056         0        NaN        NaN   \n",
      "\n",
      "        High        Low       Open  Volume  \n",
      "0        NaN        NaN        NaN     NaN  \n",
      "1        NaN        NaN        NaN     NaN  \n",
      "2  26.790001        NaN        NaN     NaN  \n",
      "3        NaN  26.393999        NaN     NaN  \n",
      "4        NaN        NaN  26.629999     NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure 'Date' column is present\n",
    "#print(data_google.columns)\n",
    "\n",
    "data_google.columns = ['_'.join(col).strip() for col in data_google.columns.values]\n",
    "\n",
    "# Inspect the DataFrame\n",
    "\n",
    "data_google = data_google.rename(columns={'Date_': 'Date'})\n",
    "\n",
    "# Melt the dataframe to long format\n",
    "data_google_long = data_google.melt(id_vars=\"Date\", var_name=\"Price_Ticker\", value_name=\"Value\")\n",
    "\n",
    "# Split the 'Price_Ticker' column into 'Price' and 'Ticker'\n",
    "data_google_long[['Price', 'Ticker']] = data_google_long['Price_Ticker'].str.split('_', expand=True)\n",
    "data_google_long = data_google_long.drop(columns=['Price_Ticker'])\n",
    "\n",
    "# Add a time_idx column\n",
    "data_google_long[\"time_idx\"] = data_google_long.groupby([\"Ticker\"]).cumcount()\n",
    "\n",
    "# Add a group_id column\n",
    "data_google_long[\"group_id\"] = data_google_long[\"Ticker\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Filter for relevant columns\n",
    "data_google_long = data_google_long[[\"Date\", \"time_idx\", \"group_id\", \"Price\", \"Value\"]]\n",
    "\n",
    "# Pivot to get each price type as a separate column\n",
    "data_google_wide = data_google_long.pivot(index=[\"Date\", \"time_idx\", \"group_id\"], columns=\"Price\", values=\"Value\").reset_index()\n",
    "\n",
    "# Rename columns\n",
    "data_google_wide.columns.name = None\n",
    "print(data_google_wide.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data type of category group_id was found to be numeric - use a string type / categorified string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m training_cutoff \u001b[38;5;241m=\u001b[39m data_google_wide[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m max_prediction_length\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Create the dataset\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m training \u001b[38;5;241m=\u001b[39m \u001b[43mTimeSeriesDataSet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_google_wide\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtraining_cutoff\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtime_idx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClose\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Predict the 'Close' price\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgroup_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_encoder_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_encoder_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_prediction_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_prediction_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_categoricals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgroup_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_reals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_varying_known_categoricals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_varying_known_reals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtime_idx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_varying_unknown_categoricals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_varying_unknown_reals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOpen\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHigh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClose\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAdj Close\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVolume\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_normalizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGroupNormalizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgroup_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msoftplus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_relative_time_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_target_scales\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_encoder_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Create validation dataset\u001b[39;00m\n\u001b[0;32m     29\u001b[0m validation \u001b[38;5;241m=\u001b[39m TimeSeriesDataSet\u001b[38;5;241m.\u001b[39mfrom_dataset(training, data_google_wide, min_prediction_idx\u001b[38;5;241m=\u001b[39mtraining_cutoff \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\deepa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_forecasting\\data\\timeseries.py:444\u001b[0m, in \u001b[0;36mTimeSeriesDataSet.__init__\u001b[1;34m(self, data, time_idx, target, group_ids, weight, max_encoder_length, min_encoder_length, min_prediction_idx, min_prediction_length, max_prediction_length, static_categoricals, static_reals, time_varying_known_categoricals, time_varying_known_reals, time_varying_unknown_categoricals, time_varying_unknown_reals, variable_groups, constant_fill_strategy, allow_missing_timesteps, lags, add_relative_time_idx, add_target_scales, add_encoder_length, target_normalizer, categorical_encoders, scalers, randomize_length, predict_mode)\u001b[0m\n\u001b[0;32m    441\u001b[0m     data\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# dummy - real value will be set dynamically in __getitem__()\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;66;03m# validate\u001b[39;00m\n\u001b[1;32m--> 444\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m data\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mis_unique, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata index has to be unique\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# add lags\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\deepa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_forecasting\\data\\timeseries.py:657\u001b[0m, in \u001b[0;36mTimeSeriesDataSet._validate_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    652\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m specified but not found in data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[0;32m    654\u001b[0m         name \u001b[38;5;129;01min\u001b[39;00m object_columns\n\u001b[0;32m    655\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (name \u001b[38;5;129;01min\u001b[39;00m category_columns \u001b[38;5;129;01mand\u001b[39;00m data[name]\u001b[38;5;241m.\u001b[39mcat\u001b[38;5;241m.\u001b[39mcategories\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbifc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    656\u001b[0m     ):\n\u001b[1;32m--> 657\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    658\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData type of category \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was found to be numeric - use a string type / categorified string\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    659\u001b[0m         )\n\u001b[0;32m    660\u001b[0m \u001b[38;5;66;03m# check for \".\" in column names\u001b[39;00m\n\u001b[0;32m    661\u001b[0m columns_with_dot \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumns[data\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "\u001b[1;31mValueError\u001b[0m: Data type of category group_id was found to be numeric - use a string type / categorified string"
     ]
    }
   ],
   "source": [
    "from pytorch_forecasting import TimeSeriesDataSet, GroupNormalizer\n",
    "\n",
    "# Define parameters\n",
    "max_prediction_length = 30  # Predict the next 30 days\n",
    "max_encoder_length = 120  # Use the past 120 days as input\n",
    "training_cutoff = data_google_wide[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "# Create the dataset\n",
    "training = TimeSeriesDataSet(\n",
    "    data_google_wide[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"Close\",  # Predict the 'Close' price\n",
    "    group_ids=[\"group_id\"],\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"group_id\"],\n",
    "    static_reals=[],\n",
    "    time_varying_known_categoricals=[],\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"],\n",
    "    target_normalizer=GroupNormalizer(groups=[\"group_id\"], transformation=\"softplus\"),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "# Create validation dataset\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data_google_wide, min_prediction_idx=training_cutoff + 1)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 64\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
